{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEg3F_5ZP3M3"
      },
      "source": [
        "#Recreate paper Twitter Sentiment Analysis\n",
        "\n",
        "---\n",
        "\n",
        "Source of paper: https://paperswithcode.com/paper/twitter-sentiment-analysis-1\n",
        "\n",
        "Source of code: https://github.com/Vedurumudi-Priyanka/Twitter-Sentiment-Analysis\n",
        "\n",
        "Source of dataset: https://github.com/shuttlesworthNEO/twitter-naive-sentiwordnet\n",
        "\n",
        "Source of glove.twitter.27b.25d.txt: https://drive.google.com/drive/folders/1LRRlFq2pgk5APevQjfV71_8LB63-YMG8?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx6p-E0ZmiZ8"
      },
      "source": [
        "Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-NCA89ylOy3",
        "outputId": "d85bdd25-5234-4860-f155-ea9a913d240e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (1.16.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (0.20.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python2.7/dist-packages (from scikit-learn) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-packages (from scikit-learn) (1.16.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python2.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-packages (from scipy) (1.16.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python2.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python2.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.16.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: scipy==1.2.2; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.0.post1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: functools32>=3.2.3; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (3.2.3.post2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (2.3.2)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.1.7)\n",
            "Requirement already satisfied: enum34>=1.1.6; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.10)\n",
            "Requirement already satisfied: mock>=2.0.0; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.8.0->tensorflow) (44.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.15.5)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0; python_version < \"3\"->tensorflow) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0; python_version < \"3\"->tensorflow) (5.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.5)\n",
            "Requirement already satisfied: rsa<4.6; python_version < \"3.6\" in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.6.16)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /usr/local/lib/python2.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.5)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python2.7/dist-packages (0.82)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python2.7/dist-packages (from xgboost) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from xgboost) (1.16.4)\n"
          ]
        }
      ],
      "source": [
        "!pip2 install numpy\n",
        "!pip2 install scikit-learn\n",
        "!pip2 install scipy\n",
        "!pip2 install nltk\n",
        "!pip2 install tensorflow\n",
        "!pip2 install xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2c5Wobo-HWT"
      },
      "source": [
        "utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9ASjgGPH-JNL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "\n",
        "def file_to_wordset(filename):\n",
        "    ''' Converts a file with a word per line to a Python set '''\n",
        "    words = []\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            words.append(line.strip())\n",
        "    return set(words)\n",
        "\n",
        "\n",
        "def write_status(i, total):\n",
        "    ''' Writes status of a process to console '''\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.write('Processing %d/%d' % (i, total))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    ''' Save list of type [(tweet_id, positive)] to csv in Kaggle format '''\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for tweet_id, pred in results:\n",
        "            csv.write(tweet_id)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "\n",
        "def top_n_words(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {word:rank} of top N words from a pickle\n",
        "    file which has a nltk FreqDist object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of words to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {word:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
        "    return words\n",
        "\n",
        "\n",
        "def top_n_bigrams(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {bigram:rank} of top N bigrams from a pickle\n",
        "    file which has a Counter object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of bigrams to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {bigram:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    bigrams = {p[0]: i for i, p in enumerate(most_common)}\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def split_data(tweets, validation_split=0.1):\n",
        "    \"\"\"Split the data into training and validation sets\n",
        "    Args:\n",
        "        tweets (list): list of tuples\n",
        "        validation_split (float, optional): validation split %\n",
        "    Returns:\n",
        "        (list, list): training-set, validation-set\n",
        "    \"\"\"\n",
        "    index = int((1 - validation_split) * len(tweets))\n",
        "    random.shuffle(tweets)\n",
        "    return tweets[:index], tweets[index:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHzKlgqhmykM"
      },
      "source": [
        "Preprocess Run preprocess.py Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2nMJ2eVm55i",
        "outputId": "7a45c1ed-73dc-416d-a238-9e7e5348c9d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 100000/100000\n",
            "Saved processed tweets to: /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/preprocess.py /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkg4MjlOoEU0"
      },
      "source": [
        "Preprocess Run preprocess.py Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCfWlbewoAWY",
        "outputId": "70b24118-d864-4867-878e-3d0653846e59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 300000/300000\n",
            "Saved processed tweets to: /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/preprocess.py /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN-wYDeMoQ-a"
      },
      "source": [
        "Preprocess Run stats.py Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcUASxvIom3j",
        "outputId": "6a0c9c21-2be7-451d-934c-92ae3673572d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 100000/100000\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 100000, Positive: 56462, Negative: 43538\n",
            "User Mentions => Total: 88955, Avg: 0.8895, Max: 12\n",
            "URLs => Total: 3599, Avg: 0.0360, Max: 4\n",
            "Emojis => Total: 1184, Positive: 997, Negative: 187, Avg: 0.0118, Max: 5\n",
            "Words => Total: 1192617, Unique: 50377, Avg: 11.9262, Max: 40, Min: 0\n",
            "Bigrams => Total: 1093149, Unique: 385105, Avg: 10.9315\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/stats.py /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhvticr6pDbR"
      },
      "source": [
        "Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt8VN4-lpFNY",
        "outputId": "9c5a23ec-3af6-48ea-d20c-a29f7703609b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct = 65.35%\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/baseline.py TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5GRf2akpRvJ"
      },
      "source": [
        "Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3kzZqZbpS4F",
        "outputId": "ff2f8cf6-13d6-42f2-a43b-e61453c8af0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7741/10000 = 77.4100 %\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/naivebayes.py TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vf4us94ppHu"
      },
      "source": [
        "Maximum Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv9RZoGqpsSL",
        "outputId": "18d17590-0242-4b9b-e69f-b38f426699e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ==> Training (1 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -0.69315        0.565\n",
            "         Final          -0.61597        0.755\n",
            "  -1.010 andyhurleyday==True and label is '0'\n",
            "   1.000 youtwitface==True and label is '0'\n",
            "   1.000 alltimelowweek==True and label is '1'\n",
            "   1.000 tired.its==True and label is '1'\n",
            "   1.000 halfbloodprince==True and label is '1'\n",
            "   1.000 hiccuuppss==True and label is '0'\n",
            "   1.000 thaipbs==True and label is '1'\n",
            "   1.000 harpersglobe==True and label is '1'\n",
            "   1.000 twitterart==True and label is '1'\n",
            "   1.000 ouchmyshoulder==True and label is '0'\n",
            "Validation set accuracy:0.6971\n",
            "\n",
            "Predicting for test data\n",
            "\n",
            "Saved to maxent.csv\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "import pickle\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "USE_BIGRAMS = False\n",
        "TRAIN = True\n",
        "\n",
        "\n",
        "def get_data_from_file(file_name, isTrain=True):\n",
        "    data = []\n",
        "    with open(train_csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if isTrain:\n",
        "                tag = line.split(',')[1]\n",
        "                bag_of_words = line.split(',')[2].split()\n",
        "                if USE_BIGRAMS:\n",
        "                    bag_of_words_bigram = list(nltk.bigrams(line.split(',')[2].split()))\n",
        "                    bag_of_words = bag_of_words+bag_of_words_bigram\n",
        "            else :\n",
        "                tag = '5'\n",
        "                bag_of_words = line.split(',')[1].split()\n",
        "                if USE_BIGRAMS:\n",
        "                    bag_of_words_bigram = list(nltk.bigrams(line.split(',')[1].split()))\n",
        "                    bag_of_words = bag_of_words+bag_of_words_bigram\n",
        "            data.append((bag_of_words, tag))\n",
        "    return data\n",
        "\n",
        "def split_data(tweets, validation_split=0.1):\n",
        "    index = int((1 - validation_split) * len(tweets))\n",
        "    random.shuffle(tweets)\n",
        "    return tweets[:index], tweets[index:]\n",
        "\n",
        "def list_to_dict(words_list):\n",
        "    return dict([(word, True) for word in words_list])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = True\n",
        "    np.random.seed(1337)\n",
        "    train_csv_file = TRAIN_PROCESSED_FILE\n",
        "    test_csv_file = TEST_PROCESSED_FILE\n",
        "    train_data = get_data_from_file(train_csv_file, isTrain=True)\n",
        "    train_set, validation_set = split_data(train_data)\n",
        "    training_set_formatted = [(list_to_dict(element[0]), element[1]) for element in train_set]\n",
        "    validation_set_formatted = [(list_to_dict(element[0]), element[1]) for element in validation_set]\n",
        "    numIterations = 1\n",
        "    algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[1]\n",
        "    classifier = nltk.MaxentClassifier.train(training_set_formatted, algorithm, max_iter=numIterations)\n",
        "    classifier.show_most_informative_features(10)\n",
        "    count = int(0)\n",
        "    for review in validation_set_formatted:\n",
        "        label = review[1]\n",
        "        text = review[0]\n",
        "        determined_label = classifier.classify(text)\n",
        "        #print(determined_label, label)\n",
        "        if determined_label!=label:\n",
        "            count+=int(1)\n",
        "    accuracy = (len(validation_set)-count)/len(validation_set)\n",
        "    print ('Validation set accuracy:%.4f'% (accuracy))\n",
        "    f = open('maxEnt_classifier.pickle', 'wb')\n",
        "    pickle.dump(classifier, f)\n",
        "    f.close()\n",
        "    print ('\\nPredicting for test data')\n",
        "    test_data = get_data_from_file(test_csv_file, isTrain=False)\n",
        "    test_set_formatted = [(list_to_dict(element[0]), element[1]) for element in test_data]\n",
        "    tweet_id = int(0)\n",
        "    results = []\n",
        "    for review in test_set_formatted:\n",
        "        text = review[0]\n",
        "        label = classifier.classify(text)\n",
        "        results.append((str(tweet_id), label))\n",
        "        tweet_id += int(1)\n",
        "    save_results_to_csv(results, 'maxent.csv')\n",
        "    print ('\\nSaved to maxent.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONNeylVGqADR"
      },
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3jKZvtCqAwK",
        "outputId": "2727c239-9344-4056-a639-b40312d421eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 6868/10000 = 68.6800 %\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/decisiontree.py TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m4doNa6qm5L"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNo_q9sCqnlE",
        "outputId": "77ca3eb9-065d-4427-e245-4c9bb80bc843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7235/10000 = 72.3500 %\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/randomforest.py TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY0oVuxDq-Ca"
      },
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4hHnE1AsEjS"
      },
      "outputs": [],
      "source": [
        "#!pip2 install --upgrade xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57VAlm3Pq-ra",
        "outputId": "efd4cf98-994a-4506-f0e1-da3fa8632c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7688/10000 = 76.8800 %\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using XGBoost.\n",
        "\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    xrange =range\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = XGBClassifier(max_depth=25, silent=False, n_estimators=400)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, 'xgboost.csv')\n",
        "        print ('\\nSaved to xgboost.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxnZ2qnarU6A"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O3WRctwrVdz",
        "outputId": "436802c4-01a4-48c1-cdc8-0f89d63e58b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7787/10000 = 77.8700 %\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/svm.py TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okXqwM7tscjq"
      },
      "source": [
        "Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0oRS7dLscTU",
        "outputId": "2f3e7a4e-5fa2-4a86-f927-215219286f0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Iteration 180/180, loss:0.4980, acc:0.7580\n",
            "Epoch: 1, val_acc:0.7575\n",
            "Accuracy improved from 0.0000 to 0.7575, saving model\n",
            "Iteration 180/180, loss:0.5031, acc:0.7880\n",
            "Epoch: 2, val_acc:0.7660\n",
            "Accuracy improved from 0.7575 to 0.7660, saving model\n",
            "Iteration 180/180, loss:0.4444, acc:0.8020\n",
            "Epoch: 3, val_acc:0.7688\n",
            "Accuracy improved from 0.7660 to 0.7688, saving model\n",
            "Iteration 180/180, loss:0.4643, acc:0.7780\n",
            "Epoch: 4, val_acc:0.7643\n",
            "Iteration 180/180, loss:0.4644, acc:0.7820\n",
            "Epoch: 5, val_acc:0.7685\n",
            "Testing\n",
            "Generating feature vectors\n",
            "Processing 300000/300000\n",
            "\n",
            "Predicting batches\n",
            "Processing 600/600\n",
            "Saved to 1layerneuralnet.csv\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Performs classification using an MLP/1-hidden-layer NN.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = False\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = np.zeros((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(500, input_dim=VOCAB_SIZE, activation='sigmoid'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_tweets):\n",
        "    correct, total = 0, len(val_tweets)\n",
        "    for val_set_X, val_set_y in extract_features(val_tweets, feat_type=FEAT_TYPE, test_file=False):\n",
        "        prediction = model.predict_on_batch(val_set_X)\n",
        "        prediction = np.round(prediction)\n",
        "        correct += np.sum(prediction == val_set_y[:, None])\n",
        "    return float(correct) / total\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    nb_epochs = 5\n",
        "    batch_size = 500\n",
        "    model = build_model()\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    best_val_acc = 0.0\n",
        "    for j in xrange(nb_epochs):\n",
        "        i = 1\n",
        "        for training_set_X, training_set_y in extract_features(train_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=False):\n",
        "            o = model.train_on_batch(training_set_X, training_set_y)\n",
        "            sys.stdout.write('\\rIteration %d/%d, loss:%.4f, acc:%.4f' %\n",
        "                             (i, n_train_batches, o[0], o[1]))\n",
        "            sys.stdout.flush()\n",
        "            i += 1\n",
        "        val_acc = evaluate_model(model, val_tweets)\n",
        "        print ('\\nEpoch: %d, val_acc:%.4f' % (j + 1, val_acc))\n",
        "        random.shuffle(train_tweets)\n",
        "        if val_acc > best_val_acc:\n",
        "            print ('Accuracy improved from %.4f to %.4f, saving model' % (best_val_acc, val_acc))\n",
        "            best_val_acc = val_acc\n",
        "            model.save('best_model.h5')\n",
        "    print ('Testing')\n",
        "    del train_tweets\n",
        "    del model\n",
        "    model = load_model('best_model.h5')\n",
        "    test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "    n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "    predictions = np.array([])\n",
        "    print ('Predicting batches')\n",
        "    i = 1\n",
        "    for test_set_X, _ in extract_features(test_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=True):\n",
        "        prediction = np.round(model.predict_on_batch(test_set_X).flatten())\n",
        "        predictions = np.concatenate((predictions, prediction))\n",
        "        write_status(i, n_test_batches)\n",
        "        i += 1\n",
        "    predictions = [(str(j), int(predictions[j]))\n",
        "                   for j in range(len(test_tweets))]\n",
        "    save_results_to_csv(predictions, '1layerneuralnet.csv')\n",
        "    print ('\\nSaved to 1layerneuralnet.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wN9SNdJtJSm"
      },
      "source": [
        "**Reccurent Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kfCmAGdtLvT",
        "outputId": "32d92d87-6828-433e-c01d-8497f365c07e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for GLOVE vectors\n",
            "Processing 1193515/0\n",
            "\n",
            "Found 38248 words in GLOVE\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "2022-01-28 05:07:19.571135: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 25)            2250025   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 25)            0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               78848     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,337,194\n",
            "Trainable params: 2,337,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "702/704 [============================>.] - ETA: 0s - loss: 0.5953 - accuracy: 0.6736\n",
            "Epoch 00001: loss improved from inf to 0.59519, saving model to /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/models/lstm.h5\n",
            "704/704 [==============================] - 15s 17ms/step - loss: 0.5952 - accuracy: 0.6737 - val_loss: 0.5156 - val_accuracy: 0.7389 - lr: 0.0010\n",
            "Epoch 2/2\n",
            "702/704 [============================>.] - ETA: 0s - loss: 0.5254 - accuracy: 0.7401\n",
            "Epoch 00002: loss improved from 0.59519 to 0.52546, saving model to /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/models/lstm.h5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.5255 - accuracy: 0.7401 - val_loss: 0.4807 - val_accuracy: 0.7645 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/lstm.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZawThNSvVDV"
      },
      "source": [
        "**CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWhD_9RhvYIi",
        "outputId": "22d5ccf1-f710-4d92-88fa-b650e06af757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for GLOVE seeds\n",
            "Processing 1193515/0\n",
            "\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "2022-01-28 05:09:27.186738: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Epoch 1/3\n",
            "704/704 [==============================] - ETA: 0s - loss: 0.5844 - accuracy: 0.6790\n",
            "Epoch 00001: loss improved from inf to 0.58443, saving model to /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/models/cnn.h5\n",
            "704/704 [==============================] - 28s 35ms/step - loss: 0.5844 - accuracy: 0.6790 - val_loss: 0.5309 - val_accuracy: 0.7192 - lr: 0.0010\n",
            "Epoch 2/3\n",
            "702/704 [============================>.] - ETA: 0s - loss: 0.5186 - accuracy: 0.7415\n",
            "Epoch 00002: loss improved from 0.58443 to 0.51862, saving model to /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/models/cnn.h5\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.5186 - accuracy: 0.7414 - val_loss: 0.4891 - val_accuracy: 0.7644 - lr: 0.0010\n",
            "Epoch 3/3\n",
            "703/704 [============================>.] - ETA: 0s - loss: 0.4894 - accuracy: 0.7625\n",
            "Epoch 00003: loss improved from 0.51862 to 0.48942, saving model to /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/models/cnn.h5\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.4894 - accuracy: 0.7625 - val_loss: 0.4775 - val_accuracy: 0.7613 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/cnn.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbC73yZev4vA"
      },
      "source": [
        "Majority Vote Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeXdysPtv5Zq",
        "outputId": "32468ced-858e-4bf4-f8e7-74aa48b5c93d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for GLOVE seeds\n",
            "Processing 1193515/0\n",
            "\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "2022-01-28 05:20:48.226536: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_input (InputLayer  [(None, 40)]             0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 40, 25)            2250025   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 25)            0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 38, 600)           45600     \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 36, 300)           540300    \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 34, 150)           135150    \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 32, 75)            33825     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2400)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 600)               1440600   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 600)               0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 600)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,445,500\n",
            "Trainable params: 4,445,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Generating feature vectors\n",
            "Processing 300000/300000\n",
            "\n",
            "293/293 [==============================] - 18s 57ms/step\n",
            "2022-01-28 05:21:32.593171: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 720000000 exceeds 10% of free system memory.\n",
            "98/98 [==============================] - 6s 62ms/step\n",
            "2022-01-28 05:21:41.994680: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 240000000 exceeds 10% of free system memory.\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/extract-cnn-feats.py /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/models/cnn.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gDNPJJuFabs",
        "outputId": "d03eb845-b57a-40d6-9429-20d0582f7d37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(90000, 600) (90000,) (10000, 600) (10000,)\n",
            "....................................................................................................\n",
            "optimization finished, #iter = 1000\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 2 may be faster (also see FAQ)\n",
            "\n",
            "Objective value = -50646.502941\n",
            "nSV = 78618\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "[LibLinear]LinearSVC(C=1, verbose=1)\n",
            "Val Accuracy: 79.92%\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/drive/MyDrive/UAS_ML/Twitter-Sentiment-Analysis-main/code/cnn-feats-svm.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPn1+rhAraqD6djCsSoMXiO",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1oc0DdXvGhiK69WVTklJdtScG3Z_swM74",
      "name": "2 UAS Success Ver 5.0 Twitter Sentiment Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
