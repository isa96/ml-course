{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCSBeO5KRV22"
      },
      "source": [
        "# 3.4.Softmax Regression\n",
        "Regression is the hammer we reach for when we want to answer how much? or how many? questions.\n",
        "\n",
        "Classification is the hammer we reach for when we want to answer “which one” questions.\n",
        "\n",
        "##3.4.1. Classification Problem\n",
        "Let us start off with a simple image classification problem. Here, each input consists of a  2×2  grayscale image. We can represent each pixel value with a single scalar, giving us four features  x1,x2,x3,x4 . Further, let us assume that each image belongs to one among the categories “cat”, “chicken”, and “dog”.\n",
        "\n",
        "Next, we have to choose how to represent the labels.\n",
        "We have two obvious choices.\n",
        "Perhaps the most natural impulse would be to choose $y \\in \\{1, 2, 3\\}$,\n",
        "where the integers represent $\\{\\text{dog}, \\text{cat}, \\text{chicken}\\}$ respectively.\n",
        "\n",
        "Fortunately, statisticians long ago invented a simple way to represent categorical data: the one-hot encoding. A one-hot encoding is a vector with as many components as we have categories. \n",
        "\n",
        "The component corresponding to particular instance's category is set to 1\n",
        "and all other components are set to 0.\n",
        "In our case, a label $y$ would be a three-dimensional vector,\n",
        "with $(1, 0, 0)$ corresponding to \"cat\", $(0, 1, 0)$ to \"chicken\",\n",
        "and $(0, 0, 1)$ to \"dog\":\n",
        "\n",
        "$$y \\in \\{(1, 0, 0), (0, 1, 0), (0, 0, 1)\\}.$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp2sTvtdl6ZF"
      },
      "source": [
        "##3.4.2. Network Architecture\n",
        "\n",
        "In order to estimate the conditional probabilities associated with all the possible classes, we need a model with multiple outputs, one per class.\n",
        "\n",
        "In our case, since we have 4 features and 3 possible output categories,\n",
        "we will need 12 scalars to represent the weights ($w$ with subscripts),\n",
        "and 3 scalars to represent the biases ($b$ with subscripts).\n",
        "We compute these three *logits*, $o_1, o_2$, and $o_3$, for each input:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\\\\n",
        "o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\\\\n",
        "o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We can depict this calculation with the neural network diagram:\n",
        "\n",
        "![Softmax regression is a single-layer neural network.](http://d2l.ai/_images/softmaxreg.svg)\n",
        "\n",
        "Since the calculation of each output,  o1,o2 , and  o3 , depends on all inputs,  x1 ,  x2 ,  x3 , and  x4 , the output layer of softmax regression can also be described as fully-connected layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOXqT2WHnDoo"
      },
      "source": [
        "##3.4.3. Parameterization Cost of Fully-Connected Layers\n",
        "\n",
        "\n",
        "However, as the name suggests,\n",
        "fully-connected layers are *fully* connected\n",
        "with potentially many learnable parameters.\n",
        "Specifically,\n",
        "for any fully-connected layer\n",
        "with $d$ inputs and $q$ outputs,\n",
        "the parameterization cost is $\\mathcal{O}(dq)$,\n",
        "which can be prohibitively high in practice.\n",
        "Fortunately,\n",
        "this cost \n",
        "of transforming $d$ inputs into $q$ outputs\n",
        "can be reduced to $\\mathcal{O}(\\frac{dq}{n})$,\n",
        "where the hyperparameter $n$ can be flexibly specified\n",
        "by us to balance between parameter saving and model effectiveness in real-world applications `Zhang.Tay.Zhang.ea.2021`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-hqKlpGnvDZ"
      },
      "source": [
        "##3.4.4. Softmax Operation\n",
        "The main approach that we are going to take here is to interpret the outputs of our model as probabilities. We will optimize our parameters to produce probabilities that maximize the likelihood of the observed data. Then, to generate predictions, we will set a threshold, for example, choosing the label with the maximum predicted probabilities.\n",
        "\n",
        "For example, if $\\hat{y}_1$, $\\hat{y}_2$, and $\\hat{y}_3$\n",
        "are 0.1, 0.8, and 0.1, respectively,\n",
        "then we predict category 2, which (in our example) represents \"chicken\".\n",
        "\n",
        "It is easy to see $\\hat{y}_1 + \\hat{y}_2 + \\hat{y}_3 = 1$\n",
        "with $0 \\leq \\hat{y}_j \\leq 1$ for all $j$.\n",
        "\n",
        "Although softmax is a nonlinear function,\n",
        "the outputs of softmax regression are still *determined* by\n",
        "an affine transformation of input features;\n",
        "thus, softmax regression is a linear model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0eYOPL8pz7y"
      },
      "source": [
        "##3.4.5. Vectorization for Minibatches\n",
        "\n",
        "To improve computational efficiency and take advantage of GPUs, we typically carry out vector calculations for minibatches of data. Assume that we are given a minibatch $\\mathbf{X}$ of examples\n",
        "with feature dimensionality (number of inputs) $d$ and batch size $n$.\n",
        "Moreover, assume that we have $q$ categories in the output.\n",
        "Then the minibatch features $\\mathbf{X}$ are in $\\mathbb{R}^{n \\times d}$,\n",
        "weights $\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$,\n",
        "and the bias satisfies $\\mathbf{b} \\in \\mathbb{R}^{1\\times q}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ6QHh_Eqci3"
      },
      "source": [
        "##3.4.6. Loss Function\n",
        "We need a loss function to measure the quality of our predicted probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s_xTd86tZ1U"
      },
      "source": [
        "##3.4.7. Information Theory Basics\n",
        "Information theory deals with the problem of encoding, decoding, transmitting, and manipulating information (also known as data) in as concise form as possible.\n",
        "\n",
        "###3.4.7.1. Entropy\n",
        "The central idea in information theory is to quantify the information content in data. \n",
        "\n",
        "###3.4.7.2. Surprisal\n",
        "If we cannot perfectly predict every event, then we might sometimes be surprised. The entropy is then the expected surprisal when one assigned the correct probabilities that truly match the data-generating process.\n",
        "\n",
        "###3.4.7.3. Cross-Entropy Revisited\n",
        "So if entropy is level of surprise experienced by someone who knows the true probability, then the cross-entropy *from* $P$ *to* $Q$, denoted $H(P, Q)$,\n",
        "is the expected surprisal of an observer with subjective probabilities $Q$\n",
        "upon seeing data that were actually generated according to probabilities $P$.\n",
        "The lowest possible cross-entropy is achieved when $P=Q$.\n",
        "\n",
        "In short, we can think of the cross-entropy classification objective in two ways: (i) as maximizing the likelihood of the observed data; and (ii) as minimizing our surprisal (and thus the number of bits) required to communicate the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdLndfvLvZjL"
      },
      "source": [
        "##3.4.8. Model Prediction and Evaluation\n",
        "After training the softmax regression model, given any example features, we can predict the probability of each output class. Normally, we use the class with the highest predicted probability as the output class. The prediction is correct if it is consistent with the actual class (label). In the next part of the experiment, we will use accuracy to evaluate the model’s performance. This is equal to the ratio between the number of correct predictions and the total number of predictions."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN0EMvh7IZb3x+fdghkdziV",
      "include_colab_link": true,
      "name": "3.4. Softmax Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
